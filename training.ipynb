{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on data file: 00000.csv\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expand_as(): argument 'other' (position 1) must be Tensor, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 42\u001b[0m\n\u001b[0;32m     40\u001b[0m         env \u001b[38;5;241m=\u001b[39m Environment(model_path, data_path, controller, debug\u001b[38;5;241m=\u001b[39mdebug)\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining on data file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 42\u001b[0m         agent\u001b[38;5;241m.\u001b[39mtrain(num_episodes, max_steps_per_episode, batch_size, env)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Evaluate the trained policy\u001b[39;00m\n\u001b[0;32m     45\u001b[0m env \u001b[38;5;241m=\u001b[39m Environment(model_path, data_path, debug\u001b[38;5;241m=\u001b[39mdebug)  \u001b[38;5;66;03m# Use the last data file for evaluation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Dylan\\Downloads\\Controls\\controls_challenge\\sac.py:235\u001b[0m, in \u001b[0;36mSAC.train\u001b[1;34m(self, num_episodes, max_steps_per_episode, batch_size, environment)\u001b[0m\n\u001b[0;32m    232\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m>\u001b[39m batch_size:\n\u001b[1;32m--> 235\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_networks(batch_size)\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Dylan\\Downloads\\Controls\\controls_challenge\\sac.py:184\u001b[0m, in \u001b[0;36mSAC.update_networks\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m    182\u001b[0m next_steering, next_acceleration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor(next_state)\n\u001b[0;32m    183\u001b[0m next_action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((next_steering, next_acceleration), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 184\u001b[0m next_log_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39mlog_prob(next_state, next_action)\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# SAC Bellman equation for calculating target Q from multiple critics.\u001b[39;00m\n\u001b[0;32m    187\u001b[0m target_Q \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscount_factor \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m terminal) \u001b[38;5;241m*\u001b[39m (\n\u001b[0;32m    188\u001b[0m     torch\u001b[38;5;241m.\u001b[39mmin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_critic(next_state, next_action), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemperature \u001b[38;5;241m*\u001b[39m next_log_prob\n\u001b[0;32m    189\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Dylan\\Downloads\\Controls\\controls_challenge\\sac.py:58\u001b[0m, in \u001b[0;36mActorNetwork.log_prob\u001b[1;34m(self, input_state, action)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_state, action):\n\u001b[0;32m     57\u001b[0m     mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(input_state)\n\u001b[1;32m---> 58\u001b[0m     log_std \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_std\u001b[38;5;241m.\u001b[39mexpand_as(mean) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_std, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_std, device\u001b[38;5;241m=\u001b[39mmean\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mexpand_as(mean)\n\u001b[0;32m     59\u001b[0m     std \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(log_std)\n\u001b[0;32m     60\u001b[0m     normal \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mNormal(mean, std)\n",
      "\u001b[1;31mTypeError\u001b[0m: expand_as(): argument 'other' (position 1) must be Tensor, not tuple"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
